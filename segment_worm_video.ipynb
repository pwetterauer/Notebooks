{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pwetterauer/Notebooks/blob/main/segment_worm_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segmenting *C. elegans* in videos with SAM 2\n",
        "\n",
        "This notebook shows how to use SAM 2 for interactive segmentation of worms in videos (adjusted from video_predictor_example from SAM2 repository). It will cover the following:\n",
        "\n",
        "- adding clicks on a frame to get and refine _masklets_ (spatio-temporal masks)\n",
        "- propagating clicks to get _masklets_ throughout the video\n",
        "- segmenting and tracking multiple objects at the same time\n",
        "\n",
        "We use the terms _segment_ or _mask_ to refer to the model prediction for an object on a single frame, and _masklet_ to refer to the spatio-temporal masks across the entire video."
      ],
      "metadata": {
        "id": "9F_LmdDCfIJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Set-up"
      ],
      "metadata": {
        "id": "rYCwJD27gR9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If running locally using jupyter, first install `sam2` in your environment using the [installation instructions](https://github.com/facebookresearch/sam2#installation) in the repository.\n",
        "\n",
        "If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'. Note that it's recommended to use **A100 or L4 GPUs when running in Colab** (T4 GPUs might also work, but could be slow and might run out of memory in some cases).\n",
        "\n",
        "It is recommended to use Google Colab, unless you have powerful hardware loacally!"
      ],
      "metadata": {
        "id": "5_0_klqugUKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using_colab = True"
      ],
      "metadata": {
        "id": "x25H__8lf8cH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'\n",
        "\n",
        "    !mkdir -p ../checkpoints/\n",
        "    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
      ],
      "metadata": {
        "id": "bEr_CaCKg3r3",
        "outputId": "c3edb14f-2c6c-4b22-8196-7cdbb57178ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "Torchvision version: 0.23.0+cu126\n",
            "CUDA is available: True\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting git+https://github.com/facebookresearch/sam2.git\n",
            "  Cloning https://github.com/facebookresearch/sam2.git to /tmp/pip-req-build-u_oi6wxv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/sam2.git /tmp/pip-req-build-u_oi6wxv\n",
            "  Resolved https://github.com/facebookresearch/sam2.git to commit 2b90b9f5ceec907a1c18123530e92e794ad901a4\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.20.1 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (4.67.1)\n",
            "Collecting hydra-core>=1.3.2 (from SAM-2==1.0)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting iopath>=0.1.10 (from SAM-2==1.0)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (11.3.0)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (25.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.10->SAM-2==1.0) (4.14.1)\n",
            "Collecting portalocker (from iopath>=0.1.10->SAM-2==1.0)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0) (6.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.5.1->SAM-2==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.5.1->SAM-2==1.0) (3.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: SAM-2, iopath\n",
            "  Building wheel for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SAM-2: filename=sam_2-1.0-cp312-cp312-linux_x86_64.whl size=473742 sha256=5959cf1a15305633260bbbb0f8ddfd3e9937b19de02432089192e174d3b3acb6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x3cws4tz/wheels/25/a3/8a/abd69dc6a6926b5e75c24810afac36c7b49b5c0f8a100147d6\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=de04539fc3c40ce5811cda67e83a68c15a0d8ecd303f76e17fc249775f398297\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built SAM-2 iopath\n",
            "Installing collected packages: portalocker, iopath, hydra-core, SAM-2\n",
            "Successfully installed SAM-2-1.0 hydra-core-1.3.2 iopath-0.1.10 portalocker-3.2.0\n",
            "--2025-08-20 15:17:10--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.15, 108.157.254.102, 108.157.254.121, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 898083611 (856M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘../checkpoints/sam2.1_hiera_large.pt’\n",
            "\n",
            "sam2.1_hiera_large. 100%[===================>] 856.48M  91.7MB/s    in 5.1s    \n",
            "\n",
            "2025-08-20 15:17:15 (169 MB/s) - ‘../checkpoints/sam2.1_hiera_large.pt’ saved [898083611/898083611]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up\n"
      ],
      "metadata": {
        "id": "YnbwUXaLg_57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# if using Apple MPS, fall back to CPU for unsupported ops\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "2sh9ELvhg-cL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select the device for computation\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    # use bfloat16 for the entire notebook\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "elif device.type == \"mps\":\n",
        "    print(\n",
        "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
        "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
        "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "BApJb7kPhHq2",
        "outputId": "9be5cbe0-2ca5-4493-f8a3-3e91849d4080",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the SAM 2 video predictor\n"
      ],
      "metadata": {
        "id": "mP0Ad2MQhO57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "\n",
        "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
      ],
      "metadata": {
        "id": "A5eZ3acchSEC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        cmap = plt.get_cmap(\"tab10\")\n",
        "        cmap_idx = 0 if obj_id is None else obj_id\n",
        "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=200):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)"
      ],
      "metadata": {
        "id": "fAfmvqaJh3fq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OYA2aRHTDu2J",
        "outputId": "9118a602-5400-4567-99eb-2aac6641df90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Select an example video\n"
      ],
      "metadata": {
        "id": "pGsJ7jwmh9BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume that the video is stored as a list of JPEG frames with filenames like `<frame_index>.jpg`.\n",
        "\n",
        "You can extract their JPEG frames using ffmpeg (https://ffmpeg.org/) as follows:\n",
        "```\n",
        "ffmpeg -i <your_video>.mp4 -q:v 2 -start_number 0 <output_dir>/'%05d.jpg'\n",
        "```\n",
        "where `-q:v` generates high-quality JPEG frames (higher number -> lower quality) and `-start_number 0` asks ffmpeg to start the JPEG file from `00000.jpg`.\n",
        "\n",
        "Upload the videos to your Google Drive, into a directory called 'video' or adjust the path to the frames in code below. Don't forget to mount your drive to Google Colab!\n"
      ],
      "metadata": {
        "id": "X0-lKFlzh_tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
        "video_dir = \"../content/drive/MyDrive/video\"\n",
        "\n",
        "# scan all the JPEG frame names in this directory\n",
        "frame_names = [\n",
        "    p for p in os.listdir(video_dir)\n",
        "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
        "]\n",
        "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
        "\n",
        "# take a look the first video frame\n",
        "frame_idx = 0\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.title(f\"frame {frame_idx}\")\n",
        "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"
      ],
      "metadata": {
        "id": "Mo0YlOibkgZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize the inference state\n"
      ],
      "metadata": {
        "id": "pir5lKmsnRoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an **inference state** on this video.\n",
        "\n",
        "During initialization, it loads all the JPEG frames in `video_path` and stores their pixels in `inference_state` (as shown in the progress bar below).\n"
      ],
      "metadata": {
        "id": "DHxcC_N7nTm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_state = predictor.init_state(video_path=video_dir)"
      ],
      "metadata": {
        "id": "iA-YRZFYnY_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Track several worms at once"
      ],
      "metadata": {
        "id": "hIzYSIJLniYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add position data for all worms. Use e.g. ImageJ to find coordinates of points inside each worm you want to track.\n",
        "\n",
        "`points` should contain one array for each worm with coordinates for points. There should be at least one point per worm, but you can add more. You can also add points outside the worm ('negative points') to better define the boundarys. Each point should be given as `[x-coordinate, y-coordinate]`.\n",
        "\n",
        "`labels` should contain one array per worm with 1 for 'positive points' and 0 for 'negative points'.\n",
        "\n",
        "Adjust `nr_of_worms` accordingly!"
      ],
      "metadata": {
        "id": "RJHBS-p-nm_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this example contains data for 4 worms in frame 0. For each worm two 'positive points' and one 'negative point' arte given.\n",
        "ann_frame_idx = 0  # the frame index we interact with\n",
        "nr_of_worms = 4    # number of worms\n",
        "worm_points = np.array([[[318, 154], [312, 170], [318, 167]],\n",
        "                   [[276, 294], [286, 322], [279, 316]],\n",
        "                   [[415, 300], [419, 325], [426, 324]],\n",
        "                   [[578, 237], [596, 266], [593, 252]]],dtype=np.float32)\n",
        "worm_labels = np.array([[1,1,0],\n",
        "                   [1,1,0],\n",
        "                   [1,1,0],\n",
        "                   [1,1,0]],np.int32)\n",
        "\n",
        "if worm_points.shape[0]==nr_of_worms:\n",
        "    print(\"Data ok!\")\n",
        "else:\n",
        "    print(\"'points' should contain one array for each worm! Check the number of worms and added points.\")"
      ],
      "metadata": {
        "id": "8DZ5zp8upVov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = {}  # hold all the data we add for visualization\n",
        "\n",
        "for i in range(nr_of_worms):\n",
        "    ann_obj_id = i+1  # give a unique id to each object we interact with\n",
        "\n",
        "    # Adds the points for one worm\n",
        "    points = worm_points[i]\n",
        "    # Adds the corresponding labels\n",
        "    labels = worm_labels[i]\n",
        "    prompts[ann_obj_id] = points, labels\n",
        "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "        inference_state=inference_state,\n",
        "        frame_idx=ann_frame_idx,\n",
        "        obj_id=ann_obj_id,\n",
        "        points=points,\n",
        "        labels=labels,\n",
        "    )\n",
        "\n",
        "# show the results on the current (interacted) frame\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.title(f\"frame {ann_frame_idx}\")\n",
        "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
        "show_points(points, labels, plt.gca())\n",
        "for i, out_obj_id in enumerate(out_obj_ids):\n",
        "    show_points(*prompts[out_obj_id], plt.gca())\n",
        "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
      ],
      "metadata": {
        "id": "n-6DZRJ_tABo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Propagate predicted masks to whole video\n"
      ],
      "metadata": {
        "id": "o5xWYLROt1Xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the results look good for the one frame, let's propagate them the whole vodeo. This will take some time."
      ],
      "metadata": {
        "id": "hWJbIwFit8VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run propagation throughout the video and collect the results in a dict\n",
        "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
        "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "    video_segments[out_frame_idx] = {\n",
        "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
        "        for i, out_obj_id in enumerate(out_obj_ids)\n",
        "    }\n",
        "\n",
        "# render the segmentation results every few frames\n",
        "vis_frame_stride = 30\n",
        "plt.close(\"all\")\n",
        "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.title(f\"frame {out_frame_idx}\")\n",
        "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
        "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
        "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
      ],
      "metadata": {
        "id": "rG0eoTPRuSft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the results"
      ],
      "metadata": {
        "id": "G5fQ4kFhuZ99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The segmentation masks can be stored in a `results.pk` file. Additionally, the figures visualising the result in single frames can be saved."
      ],
      "metadata": {
        "id": "Pp16s_-NwIYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "mlZqLVETwi-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/result.pk', , 'wb') as f:\n",
        "    pickle.dump(video_segments, f)"
      ],
      "metadata": {
        "id": "gScUvgaCwUXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shows and saves frame 450 with overlayed masks as png file\n",
        "frame_nr = 450\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "ax.set_title(f\"frame {frame_nr}\")\n",
        "ax.set_axis_off()\n",
        "ax.imshow(Image.open(os.path.join(video_dir, frame_names[frame_nr])))\n",
        "for out_obj_id, out_mask in video_segments[frame_nr].items():\n",
        "    show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n",
        "fig.savefig(\"../content/drive/MyDrive/frame_with_masks.png\")"
      ],
      "metadata": {
        "id": "gRpmNQ9FxvJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the saved results"
      ],
      "metadata": {
        "id": "2sk7a4zL78S0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following cell to load pickled results back into `video_segments` variable."
      ],
      "metadata": {
        "id": "byKxI17f8CEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/result.pk', 'rb') as f:\n",
        "  video_segments=pickle.load(f)"
      ],
      "metadata": {
        "id": "lQEASM0H7cbp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}